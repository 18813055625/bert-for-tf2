BERT for TensorFlow v2
======================


This repo contains a TensorFlow v2 Keras implementation of `google-research/bert`_,
with support for load the original `pre-trained weights`_,
and producing numerically identical activations.



Resources
---------

- `BERT`_ - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- `google-research/bert`_ - the original BERT implementation
- `kpe/params-flow`_ - utilities for reducing keras boilerplate code in custom layers

.. _`pre-trained weights`: https://github.com/google-research/bert#pre-trained-models
.. _`google-research/bert`: https://github.com/google-research/bert
.. _`BERT`: https://arxiv.org/abs/1810.04805
.. _`kpe/params-flow`: https://github.com/kpe/params-flow

